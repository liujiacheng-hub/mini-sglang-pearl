from __future__ import annotations

from dataclasses import dataclass
from functools import cached_property
from typing import TYPE_CHECKING, List

import torch
from minisgl.distributed import DistributedInfo
from minisgl.utils import cached_load_hf_config

if TYPE_CHECKING:
    from minisgl.models import ModelConfig


@dataclass(frozen=True)
class EngineConfig:
    model_path: str
    tp_info: DistributedInfo
    dtype: torch.dtype
    max_running_req: int = 256
    attention_backend: str = "auto"
    cuda_graph_bs: List[int] | None = None
    cuda_graph_max_bs: int | None = None
    page_size: int = 1
    memory_ratio: float = 0.9
    distributed_timeout: float = 60.0
    use_dummy_weight: bool = False
    use_pynccl: bool = True
    max_seq_len_override: int | None = None
    num_page_override: int | None = None  # if not None, will override the number of pages
    # TODO(ljc)
    enable_pearl: bool = False
    draft_model_path: str | None = None
    tp_size_target: int = 1
    tp_size_draft: int = 0

    # TODO(ljc)
    @cached_property
    def hf_config(self):
        if self.enable_pearl and self.tp_info.is_draft():
            return cached_load_hf_config(self.draft_model_path)
        return cached_load_hf_config(self.model_path)

    @cached_property
    def model_config(self) -> ModelConfig:
        from minisgl.models import ModelConfig

        return ModelConfig.from_hf(self.hf_config)

    @property
    def max_seq_len(self) -> int:
        if self.max_seq_len_override is not None:
            return self.max_seq_len_override
        return self.model_config.rotary_config.max_position

    @property
    def max_forward_len(self) -> int:
        return self.max_seq_len

    @property
    def distributed_addr(self) -> str:
        return "tcp://127.0.0.1:23333"

    # TODO(ljc)
    @property
    def target_model_devices(self) -> List[int]:
        return list(range(0, self.tp_size_target))
    
    @property
    def draft_model_devices(self) -> List[int]:
        return list(range(self.tp_size_target, self.tp_size_target + self.tp_size_draft))